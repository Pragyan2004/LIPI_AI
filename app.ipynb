{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb49b20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Pragyan\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Pragyan\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7340046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 1164823\n",
      "Unique words: 66684\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_name = \"final.txt\"   # make sure this file exists in same folder\n",
    "\n",
    "with open(file_name, 'r', encoding=\"utf8\") as f:\n",
    "    text_data = f.read().lower()\n",
    "    words = re.findall(r'\\w+', text_data)\n",
    "\n",
    "vocab = set(words)\n",
    "\n",
    "print(\"Total words:\", len(words))\n",
    "print(\"Unique words:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96edbdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_frequency(words):\n",
    "    word_count = {}\n",
    "    for word in words:\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "    return word_count\n",
    "\n",
    "word_count = count_word_frequency(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d2e1b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': 3, 'o': 6, 'r': 3, 'l': 4, 'd': 3, ' ': 5, 'i': 1, 's': 2, 'c': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_word_frequency(\"world world is so cool world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19a06ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(word_count):\n",
    "    total_words = sum(word_count.values())\n",
    "    return {word: count / total_words for word, count in word_count.items()}\n",
    "\n",
    "probabilities = calculate_probability(word_count)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    \"\"\"Lemmatize a given word using NLTK WordNet Lemmatizer.\"\"\"\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "def delete_letter(word):\n",
    "    return [word[:i] + word[i+1:] for i in range(len(word))]\n",
    "\n",
    "def swap_letters(word):\n",
    "    return [word[:i] + word[i+1] + word[i] + word[i+2:] for i in range(len(word)-1)]\n",
    "\n",
    "def replace_letter(word):\n",
    "    letters = string.ascii_lowercase\n",
    "    return [word[:i] + l + word[i+1:] for i in range(len(word)) for l in letters]\n",
    "\n",
    "def insert_vletter(word):\n",
    "    letters = string.ascii_lowercase\n",
    "    return [word[:i] + l + word[i:] for i in range(len(word)+1) for l in letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b380d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(word):\n",
    "    candidates = set()\n",
    "    candidates.update(delete_letter(word))\n",
    "    candidates.update(swap_letters(word))\n",
    "    candidates.update(replace_letter(word))\n",
    "    candidates.update(insert_letter(word))\n",
    "    return candidates\n",
    "\n",
    "def generate_candidates_level2(word):\n",
    "    level1 = generate_candidates(word)\n",
    "    level2 = set()\n",
    "    for w in level1:\n",
    "        level2.update(generate_candidates(w))\n",
    "    return level2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_correction(word, probs, vocab, max_suggestions=5):\n",
    "    candidates = (\n",
    "        [word] if word in vocab else list(generate_candidates(word).intersection(vocab)) or\n",
    "        list(generate_candidates_level2(word).intersection(vocab))\n",
    "    )\n",
    "    return sorted([(w, probs.get(w, 0)) for w in candidates], key=lambda x: x[1], reverse=True)[:max_suggestions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98ea4ab8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'insert_letter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Enter a word for autocorrection: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m suggestions \u001b[38;5;241m=\u001b[39m \u001b[43mget_best_correction\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_suggestions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Top suggestions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m suggestion \u001b[38;5;129;01min\u001b[39;00m suggestions:\n",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m, in \u001b[0;36mget_best_correction\u001b[1;34m(word, probs, vocab, max_suggestions)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_best_correction\u001b[39m(word, probs, vocab, max_suggestions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      2\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m----> 3\u001b[0m         [word] \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m vocab \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mgenerate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mintersection(vocab)) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mlist\u001b[39m(generate_candidates_level2(word)\u001b[38;5;241m.\u001b[39mintersection(vocab))\n\u001b[0;32m      5\u001b[0m     )\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m([(w, probs\u001b[38;5;241m.\u001b[39mget(w, \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m candidates], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:max_suggestions]\n",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m, in \u001b[0;36mgenerate_candidates\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      4\u001b[0m candidates\u001b[38;5;241m.\u001b[39mupdate(swap_letters(word))\n\u001b[0;32m      5\u001b[0m candidates\u001b[38;5;241m.\u001b[39mupdate(replace_letter(word))\n\u001b[1;32m----> 6\u001b[0m candidates\u001b[38;5;241m.\u001b[39mupdate(\u001b[43minsert_letter\u001b[49m(word))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m candidates\n",
      "\u001b[1;31mNameError\u001b[0m: name 'insert_letter' is not defined"
     ]
    }
   ],
   "source": [
    "user_input = input(\"\\n Enter a word for autocorrection: \")\n",
    "suggestions = get_best_correction(user_input, probabilities, vocab, max_suggestions=5)\n",
    "\n",
    "print(\"\\n Top suggestions:\")\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "462886ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Pragyan\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Pragyan\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Pragyan\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Pragyan\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words loaded: 1164823\n",
      "Unique words: 66684\n",
      "Model saved to autocorrect_model.pkl\n",
      "\n",
      "Testing Autocorrect System:\n",
      "\n",
      "Word: worls\n",
      "Suggestions: ['words', 'world', 'works', 'worlds', 'worms']\n",
      "\n",
      "Word: programmingg\n",
      "Suggestions: ['programming']\n",
      "\n",
      "Word: pythoon\n",
      "Suggestions: []\n",
      "\n",
      "Word: computr\n",
      "Suggestions: ['computer', 'compute']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class AutoCorrectSystem:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.word_count = {}\n",
    "        self.probabilities = {}\n",
    "        self.vocab = set()\n",
    "        self.total_words = 0\n",
    "        \n",
    "    def load_text_file(self, file_name):\n",
    "        \"\"\"Load and process text file\"\"\"\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=\"utf8\") as f:\n",
    "                text_data = f.read().lower()\n",
    "                words = re.findall(r'\\w+', text_data)\n",
    "                \n",
    "            self.vocab = set(words)\n",
    "            self.total_words = len(words)\n",
    "            self.word_count = self.count_word_frequency(words)\n",
    "            self.probabilities = self.calculate_probability()\n",
    "            \n",
    "            print(f\"Total words loaded: {self.total_words}\")\n",
    "            print(f\"Unique words: {len(self.vocab)}\")\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: {file_name} not found!\")\n",
    "            return False\n",
    "    \n",
    "    def count_word_frequency(self, words):\n",
    "        \"\"\"Count frequency of each word\"\"\"\n",
    "        return dict(Counter(words))\n",
    "    \n",
    "    def calculate_probability(self):\n",
    "        \"\"\"Calculate probability of each word\"\"\"\n",
    "        total = sum(self.word_count.values())\n",
    "        return {word: count/total for word, count in self.word_count.items()}\n",
    "    \n",
    "    def lemmatize_word(self, word):\n",
    "        \"\"\"Lemmatize a given word\"\"\"\n",
    "        return self.lemmatizer.lemmatize(word)\n",
    "    \n",
    "    def delete_letter(self, word):\n",
    "        \"\"\"Generate strings by deleting one character\"\"\"\n",
    "        return [word[:i] + word[i+1:] for i in range(len(word))]\n",
    "    \n",
    "    def swap_letters(self, word):\n",
    "        \"\"\"Generate strings by swapping adjacent characters\"\"\"\n",
    "        return [word[:i] + word[i+1] + word[i] + word[i+2:] \n",
    "                for i in range(len(word)-1)]\n",
    "    \n",
    "    def replace_letter(self, word):\n",
    "        \"\"\"Generate strings by replacing one character\"\"\"\n",
    "        letters = string.ascii_lowercase\n",
    "        return [word[:i] + l + word[i+1:] \n",
    "                for i in range(len(word)) for l in letters]\n",
    "    \n",
    "    def insert_letter(self, word):\n",
    "        \"\"\"Generate strings by inserting one character\"\"\"\n",
    "        letters = string.ascii_lowercase\n",
    "        return [word[:i] + l + word[i:] \n",
    "                for i in range(len(word)+1) for l in letters]\n",
    "    \n",
    "    def generate_candidates_level1(self, word):\n",
    "        \"\"\"Generate all possible level 1 candidates\"\"\"\n",
    "        candidates = set()\n",
    "        candidates.update(self.delete_letter(word))\n",
    "        candidates.update(self.swap_letters(word))\n",
    "        candidates.update(self.replace_letter(word))\n",
    "        candidates.update(self.insert_letter(word))\n",
    "        return candidates\n",
    "    \n",
    "    def generate_candidates_level2(self, word):\n",
    "        \"\"\"Generate level 2 candidates\"\"\"\n",
    "        level1 = self.generate_candidates_level1(word)\n",
    "        level2 = set()\n",
    "        for w in level1:\n",
    "            level2.update(self.generate_candidates_level1(w))\n",
    "        return level2\n",
    "    \n",
    "    def get_best_correction(self, word, max_suggestions=5, use_lemmatization=True):\n",
    "        \"\"\"Get best corrections for a word\"\"\"\n",
    "        if use_lemmatization:\n",
    "            word = self.lemmatize_word(word)\n",
    "        \n",
    "        # Check if word is in vocabulary\n",
    "        if word in self.vocab:\n",
    "            return [(word, self.probabilities.get(word, 0))]\n",
    "        \n",
    "        # Generate level 1 candidates\n",
    "        candidates = self.generate_candidates_level1(word)\n",
    "        valid_candidates = candidates.intersection(self.vocab)\n",
    "        \n",
    "        # If no level 1 candidates, try level 2\n",
    "        if not valid_candidates:\n",
    "            candidates = self.generate_candidates_level2(word)\n",
    "            valid_candidates = candidates.intersection(self.vocab)\n",
    "        \n",
    "        # Sort by probability\n",
    "        suggestions = [(w, self.probabilities.get(w, 0)) \n",
    "                      for w in valid_candidates]\n",
    "        suggestions.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return suggestions[:max_suggestions]\n",
    "    \n",
    "    def add_word_to_vocab(self, word):\n",
    "        \"\"\"Add a new word to vocabulary\"\"\"\n",
    "        word = word.lower()\n",
    "        if word not in self.vocab:\n",
    "            self.vocab.add(word)\n",
    "            self.word_count[word] = self.word_count.get(word, 0) + 1\n",
    "            self.total_words += 1\n",
    "            self.probabilities = self.calculate_probability()\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_word_stats(self, word):\n",
    "        \"\"\"Get statistics for a word\"\"\"\n",
    "        word = word.lower()\n",
    "        if word in self.word_count:\n",
    "            return {\n",
    "                'word': word,\n",
    "                'frequency': self.word_count[word],\n",
    "                'probability': self.probabilities[word],\n",
    "                'in_vocab': True\n",
    "            }\n",
    "        return {\n",
    "            'word': word,\n",
    "            'frequency': 0,\n",
    "            'probability': 0,\n",
    "            'in_vocab': False\n",
    "        }\n",
    "    \n",
    "    def save_model(self, filename='autocorrect_model.pkl'):\n",
    "        \"\"\"Save the model to a pickle file\"\"\"\n",
    "        model_data = {\n",
    "            'word_count': self.word_count,\n",
    "            'probabilities': self.probabilities,\n",
    "            'vocab': self.vocab,\n",
    "            'total_words': self.total_words\n",
    "        }\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "    \n",
    "    def load_model(self, filename='autocorrect_model.pkl'):\n",
    "        \"\"\"Load the model from a pickle file\"\"\"\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            self.word_count = model_data['word_count']\n",
    "            self.probabilities = model_data['probabilities']\n",
    "            self.vocab = model_data['vocab']\n",
    "            self.total_words = model_data['total_words']\n",
    "            print(f\"Model loaded from {filename}\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Create and save the model\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the system\n",
    "    autocorrect = AutoCorrectSystem()\n",
    "    \n",
    "    # Load your text file\n",
    "    if autocorrect.load_text_file(\"final.txt\"):\n",
    "        # Save the model\n",
    "        autocorrect.save_model('autocorrect_model.pkl')\n",
    "        \n",
    "        # Test the system\n",
    "        test_words = [\"worls\", \"programmingg\", \"pythoon\", \"computr\"]\n",
    "        print(\"\\nTesting Autocorrect System:\")\n",
    "        for word in test_words:\n",
    "            suggestions = autocorrect.get_best_correction(word)\n",
    "            print(f\"\\nWord: {word}\")\n",
    "            print(f\"Suggestions: {[s[0] for s in suggestions]}\")\n",
    "    else:\n",
    "        print(\"Please ensure 'final.txt' exists in the current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2429ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
